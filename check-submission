hi#! /usr/bin/env ruby

require 'net/http'
require 'nokogiri'
require 'uri'
require 'json'

SITES = [
  ["Analog", "https://analogsf.com/contact-us/writers-guidelines/"],
  ["Andromeda Spaceways", "https://andromedaspaceways.com/submissions-manager/"],
  ["Apex Magazine", "https://www.apexbookcompany.com/a/blog/apex-magazine/post/apex-magazine-submissions-guidelines"],
  ["Asimov's", "https://asimovs.com/contact-us/writers-guidelines/"],
  ["Augur", "https://augursociety.org/submissions/"],
  ["Aurealis", "https://aurealis.com.au/submissions/"],
  ["Beneath Ceaseless Skies", "https://www.beneath-ceaseless-skies.com/submissions/"],
  ["Clarkesworld", "https://clarkesworldmagazine.com/submissions/"],
  ["Daily Science Fiction", "https://dailysciencefiction.com/"],
  ["Diabolical Plots", "https://www.diabolicalplots.com/guidelines/"],
  ["Every Day Fiction", "https://everydayfiction.com/submit-story/"],
  ["F(r)iction", "https://frictionlit.org/about/submit/"],
  ["Fantasy & Science Fiction", "https://www.sfsite.com/fsf/glines.htm"],
  ["Fantasy Magazine", "https://psychopomp.com/fantasy-magazine-guidelines/"],
  ["Fusion Fragment", "https://www.fusionfragment.com/submissions/"],
  ["Galaxy's Edge", "https://www.galaxysedge.com/submissions/"],
  ["Interzone", "https://interzone.press/submissions/"],
  ["Lightspeed", "https://adamant.moksha.io/publication/lightspeed"],
  ["Neon Dystopia", "https://www.neondystopia.com/?p=100042814"],
  ["Nightmare", "https://adamant.moksha.io/publication/nightmare/guidelines"],
  ["Penumbric", "https://www.penumbric.com/subs.html"],
  ["Psychopomp Novellas", "https://psychopomp.com/novella-guidelines/"],
  ["Shoreline of Infinity", "https://www.shorelineofinfinity.com/submissions/"],
  ["Small Wonders", "https://smallwondersmag.com/submissions/"],
  ["Strange Horizons", "http://strangehorizons.com/submit/fiction-submission-guidelines/"],
  ["The Deadlands", "https://psychopomp.com/the-deadlands-guidelines/"],
  ["Three-Lobed Burning Eye", "https://www.3lobedmag.com/submissions.html"],
  ["Trembling with Fear", "https://horrortree.com/submissions/"],
  ["Uncanny", "https://www.uncannymagazine.com/submissions/"],
  ["Weird Tales", "https://www.weirdtales.com/submissions"],
  ["Worlds of If", "https://worldsofifmagazine.com/submissions"],
  ["Worlds of Possibility", "https://worldsofpossibility.moksha.io/closed"],
  ["swamp pink", "https://swamp-pink.charleston.edu/submit/"],
]

GITHUB_TOKEN = ENV['GITHUB_TOKEN']
MODEL = "openai/gpt-4.1"

abort "Missing GITHUB_TOKEN." unless GITHUB_TOKEN && !GITHUB_TOKEN.empty?

def fetch_url(url)
  uri = URI.parse(url)
  Net::HTTP.start(uri.host, uri.port, use_ssl: uri.scheme == 'https') do |http|
    req = Net::HTTP::Get.new(uri)
    response = http.request(req)
    if response.is_a?(Net::HTTPRedirection)
      location = response['location']
      return fetch_url(location)
    elsif !response.is_a?(Net::HTTPSuccess)
      raise "Failed to fetch #{url}: #{response.code}"
    end
    response.body
  end
end

def extract_main_text(html, max_length = 16_000)
  doc = Nokogiri::HTML(html)

  # Try to find 'main' first, then 'article', then the largest section/div
  main_node = doc.at('main') || doc.at('article')
  unless main_node
    # Heuristic: Find the <div> or <section> with the most <p> tags
    candidates = doc.css('div, section')
    main_node = candidates.max_by { |c| c.css('p').size }
  end
  main_node ||= doc.at('body') # fallback

  # Remove navigation, header, footer, aside and scripts/styles
  main_node.search('header, nav, footer, aside, script, style, noscript').remove

  # Get the visible text, squeeze spaces, strip, and join paragraphs.
  text = main_node.css('p, h1, h2, h3, li').map(&:text).join("\n").gsub(/\s+/, ' ').strip

  # Truncate to max_length, ideally at a sentence boundary.
  if text.length > max_length
    # Cut at the last period before the limit, if found, else hard cut
    idx = text.rindex('.', max_length) || max_length
    text = text[0, idx].strip
    text << "..." if text.length >= max_length
  end

  text
end

def ask_github_models(prompt, api_key, model)
  uri = URI("https://models.github.ai/inference/chat/completions")
  headers = {
    "Authorization" => "Bearer #{api_key}",
    "Content-Type" => "application/json",
    "User-Agent" => "ruby-openai-check"
  }
  body = {
    "model" => model,
    "messages" => [
      {"role" => "system", "content" => "You are an assistant that summarizes submission status for literary magazines."},
      {"role" => "user", "content" => prompt}
    ],
    "max_tokens" => 256,
    "temperature" => 0.1
  }.to_json

  Net::HTTP.start(uri.host, uri.port, use_ssl: true) do |http|
    req = Net::HTTP::Post.new(uri, headers)
    req.body = body
    res = http.request(req)
    unless res.is_a?(Net::HTTPSuccess)
      abort "API Error: #{res.code} #{res.body}"
    end
    response_json = JSON.parse(res.body)
    # Look for OpenAI-style completion
    response_json.dig("choices", 0, "message", "content") || "(No response)"
  end
end

def output_rows(rows, icon)
  rows.sort_by {|(title, page, *_)| title }.each do |(title, page, _, reason)|
    puts "|[#{title}](#{page})|#{icon}|<details>#{reason}</details>|"
  end
end

# Main script starts here

results = []

sites = ARGV.empty? ? SITES : ARGV.map { |site| ["", site] }
sites.each do |(title, page)|
  STDERR.print "."
  raw_content = fetch_url(page)
  page_content = extract_main_text(raw_content)

  prompt = <<~PROMPT
    This is the submission guideline page for a magazine.
    Based on its current content, is the magazine currently OPEN for fiction submissions?
    Only answer "YES" or "NO" then after a newline provide a brief reason.
    Here is the page content:

    #{page_content}
  PROMPT

  response = ask_github_models(prompt, GITHUB_TOKEN, MODEL)

  answer, reason = response.split("\n").reject { |line| line.nil? || line.empty? }

  results << [title, page, answer, reason]
rescue => e
  STDERR.print "!"
  result << [title, page, "ERROR", e.message]
end

puts "# Submission Tracker"
puts
puts "Just a few magazines I'm watching for open submission season"
puts
puts "|Site|Open?|Why???|"
puts "|---|---|---|"

by_answer = results.group_by { |(title, page, answer, reason)| answer }
yup = by_answer["YES"] || []
nope = by_answer["NO"] || []

output_rows(yup, "ðŸ’š")
output_rows(nope, "ðŸ›‘")
