#! /usr/bin/env ruby

require 'net/http'
require 'nokogiri'
require 'uri'
require 'json'

require_relative './sites'

GITHUB_TOKEN = ENV['GITHUB_TOKEN']
MODEL = "openai/gpt-4.1"

abort "Missing GITHUB_TOKEN." unless GITHUB_TOKEN && !GITHUB_TOKEN.empty?

def fetch_url(url)
  uri = URI.parse(url)
  Net::HTTP.start(uri.host, uri.port, use_ssl: uri.scheme == 'https') do |http|
    req = Net::HTTP::Get.new(uri)
    response = http.request(req)
    if response.is_a?(Net::HTTPRedirection)
      location = response['location']
      return fetch_url(location)
    elsif !response.is_a?(Net::HTTPSuccess)
      raise "Failed to fetch #{url}: #{response.code}"
    end
    response.body
  end
end

def extract_main_text(html, max_length = 16_000)
  doc = Nokogiri::HTML(html)

  # Try to find 'main' first, then 'article', then the largest section/div
  main_node = doc.at('main') || doc.at('article')
  unless main_node
    # Heuristic: Find the <div> or <section> with the most <p> tags
    candidates = doc.css('div, section')
    main_node = candidates.max_by { |c| c.css('p').size }
  end
  main_node ||= doc.at('body') # fallback
  main_node ||= doc

  # Remove navigation, header, footer, aside and scripts/styles
  main_node.search('header, nav, footer, aside, script, style, noscript').remove

  # Get the visible text, squeeze spaces, strip, and join paragraphs.
  text = main_node.css('p, h1, h2, h3, li').map(&:text).join("\n").gsub(/\s+/, ' ').strip

  # Truncate to max_length, ideally at a sentence boundary.
  if text.length > max_length
    # Cut at the last period before the limit, if found, else hard cut
    idx = text.rindex('.', max_length) || max_length
    text = text[0, idx].strip
    text << "..." if text.length >= max_length
  end

  text
end

def ask_github_models(prompt, api_key, model)
  uri = URI("https://models.github.ai/inference/chat/completions")
  headers = {
    "Authorization" => "Bearer #{api_key}",
    "Content-Type" => "application/json",
    "User-Agent" => "ruby-openai-check"
  }
  body = {
    "model" => model,
    "messages" => [
      {"role" => "system", "content" => "You are an assistant that summarizes submission status for literary magazines."},
      {"role" => "user", "content" => prompt}
    ],
    "max_completion_tokens" => 256,
  }.to_json

  Net::HTTP.start(uri.host, uri.port, use_ssl: true) do |http|
    req = Net::HTTP::Post.new(uri, headers)
    req.body = body
    res = http.request(req)
    unless res.is_a?(Net::HTTPSuccess)
      throw "API Error: #{res.code} #{res.body}"
    end
    response_json = JSON.parse(res.body)
    # Look for OpenAI-style completion
    response_json.dig("choices", 0, "message", "content") || "(No response)"
  end
end

def evaluate_site(results, title, page, tags)
  STDERR.print "."
  raw_content = fetch_url(page)
  page_content = extract_main_text(raw_content)

  prompt = <<~PROMPT
    This is the submission guideline page for a magazine.
    Based on its current content, is the magazine currently OPEN for fiction submissions?
    Only answer "YES" or "NO" then after a newline provide a brief reason.
    Here is the page content:

    #{page_content}
  PROMPT

  response = ask_github_models(prompt, GITHUB_TOKEN, MODEL)

  answer, reason = response.split("\n").reject { |line| line.nil? || line.empty? }
  if answer.nil? || answer.empty?
    answer = "ERROR"
    reason = "No response from model?"
  end

  results << [title, page, answer, reason, tags]
rescue => e
  STDERR.print "!"
  results << [title, page, "ERROR", e.message, tags]
ensure
  STDERR.flush
end

def output_results(results)
  by_answer = results.group_by { |(title, page, answer, reason)| answer }
  yup = by_answer["YES"] || []
  nope = by_answer["NO"] || []
  errors = by_answer["ERROR"] || []

  output_rows(yup, "ðŸ’š")
  output_rows(nope, "ðŸ›‘")
  output_rows(errors, "ðŸ’¥")
end

def output_rows(rows, icon)
  rows
    .sort_by {|(title, page, *_)| title }
    .each do |(title, page, _, reason, tags)|
      puts "|[#{title}](#{page})|#{icon}|#{tags}|<details>#{reason}</details>|"
    end
end

# Main script starts here

results = []

if ARGV.empty?
  sites = SITES.group_by { |(_, _, tags)| tags.include?("$") }
  paying_sites = sites[true] || []
  nonpaying_sites = sites[false] || []

  paying_sites
    .sort_by { |(title, *_)| title }
    .each { |(title, page, tags)| evaluate_site(results, title, page, tags) }

  puts "# Submission Tracker"
  puts
  puts "Just a few magazines I'm watching for open submission season"
  puts
  puts "|Site|Open?|Tag Along!|Why???|"
  puts "|---|---|---|---|"

  output_results(results)

  puts ""
  puts "## Maybe Later"
  puts "|Site|Tag Along!|"
  puts "|---|---|"
  nonpaying_sites.each do |(title, page, tags)|
    puts "|[#{title}](#{page})|#{tags}|"
  end
else
  ARGV.map do |site|
    evaluate_site(results, "", site, "")
  end
  output_results(results)
end
